# AI and ML Research/Experience

Work in AI roughly falls into three main categories:

- Research on the algorithms themselves
- Application of AI/ML techniques to a specific task
- Deployment and upscaling of an AI application

These categories are by no means discrete. Rather, they should be viewed as a continuous spectrum.


# Research on the algorithms and techniques themselves

As the name suggests, research is being done on creating new AI techniques as well as improving existing algorithms. Of the three categories, this one requires the strongest foundation in mathematics. Examples of this include the advent of the perceptron and neural nets, the creation of the transformer architecture, and AI alignment research.


[TBD. More to added later]


# Application of AI/ML techniques to a specific task

Whether it is advertisement, playing go, or computer vision, AI can be applied to a host of fields, and that list of fields is constantly growing. At Columbia University, there are mainly research labs that are currently applying AI to biomedical research, be it genetics, drug development, or another healthcare related topic. The field as a whole shows much promise, especially with the usage of Deepmind’s AlphaFold2.

Another example is Hugging Face, which is a platform with advanced tools for natural language processing (NLP). One can use it to summarize pages of text, derive sentiment from text, auto complete one’s writing, and much more. 


[TBD. More to added later]

# Deployment and upscaling of an AI application

While AI is a group of powerful tools, the vast majority of resources that institutions and companies put forth into AI never make it to production. One of the factors is artificial intelligence requires a fair amount of infrastructure to work. This includes lots of computing power, data wrangling, data logistics, and continuous integration plus continuous deployment (CICD). In conjunction with buzzwords such as DevOps and DevSecOps, MLOps is becoming popular, and it is meant to embody these practices as well as account for the required infrastructure.

An example of this is Chick-Fil-A. At every one of their locations, they have a small Intel Nuc computer that takes in data from their tools. This data can be sent back through the Cloud to their central location, and their models and analytics are updated and retrained accordingly. The models can be packaged up (e.g. Kubernetes) and deployed again to each location for usage. The models can be tailored with specific characteristics, such as accounting for certain location/region and the accompanying variables.

Aside from organizations developing and applying AI techniques internally, there are companies provide AI/ML platforms as a service. Some of these provide the accompanying infrastructure, letting the data scientist focus just on the AI itself, while others provide a host of pretrained and suffocated models.

[TBD. More to added later]